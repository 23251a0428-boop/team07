{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"machine_shape":"hm","gpuType":"T4","provenance":[{"file_id":"https://huggingface.co/ibm-granite/granite-3.3-2b-instruct.ipynb","timestamp":1756958177612}]},"accelerator":"GPU","kaggle":{"accelerator":"gpu"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","source":["!pip install -U transformers"],"metadata":{"id":"7lrA8EPt2u4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Local Inference on GPU\n","Model page: https://huggingface.co/ibm-granite/granite-3.3-2b-instruct\n","\n","‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-3.3-2b-instruct)\n","\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"],"metadata":{"id":"o6JFnPNF2u4h"}},{"cell_type":"code","source":["# ------------------ Imports ------------------\n","import io, re\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize\n","from sklearn.metrics.pairwise import cosine_similarity\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n","from sentence_transformers import SentenceTransformer\n","\n","# ------------------ NLTK Setup ------------------\n","nltk.download(\"punkt\", quiet=True)\n","nltk.download(\"stopwords\", quiet=True)\n","STOP = set(stopwords.words(\"english\"))\n","\n","# ------------------ Text Preprocessing ------------------\n","def chunk_text(text, max_chars=1000, overlap=100):\n","    chunks, start, n = [], 0, len(text)\n","    while start < n:\n","        end = min(start + max_chars, n)\n","        period = text.rfind(\".\", start, end)\n","        if period > start + 200:\n","            end = period + 1\n","        chunks.append(text[start:end].strip())\n","        start = end - overlap\n","    return chunks\n","\n","# ------------------ Embedding + Retrieval ------------------\n","def load_embedder():\n","    return SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","def build_index(embedder, chunks):\n","    embs = embedder.encode(chunks)\n","    return embs, chunks\n","\n","def retrieve(query, embedder, embs, chunks, top_k=5):\n","    q_emb = embedder.encode([query])\n","    sims = cosine_similarity(q_emb, embs)[0]\n","    top_indices = sims.argsort()[::-1][:top_k]\n","    return [(chunks[i], float(sims[i])) for i in top_indices]\n","\n","# ------------------ Granite Model ------------------\n","def load_granite():\n","    model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        device_map=\"auto\",\n","        torch_dtype=dtype\n","    )\n","    return tokenizer, model\n","\n","def granite_generate(prompt, tokenizer, model, max_tokens=256):\n","    conversation = [{\"role\": \"user\", \"content\": prompt}]\n","    inputs = tokenizer.apply_chat_template(\n","        conversation,\n","        return_tensors=\"pt\",\n","        thinking=True,\n","        return_dict=True,\n","        add_generation_prompt=True\n","    ).to(model.device)\n","    set_seed(42)\n","    output = model.generate(**inputs, max_new_tokens=max_tokens)\n","    response = tokenizer.decode(\n","        output[0, inputs[\"input_ids\"].shape[1]:],\n","        skip_special_tokens=True\n","    )\n","    return response.strip()\n","\n","# ------------------ Summarization ------------------\n","def summarize_text(text, tokenizer, model):\n","    prompt = f\"Summarize the following text:\\n{text}\\nSummary:\"\n","    return granite_generate(prompt, tokenizer, model)\n","\n","# ------------------ Quiz Generator ------------------\n","def make_fill_blank(sentence):\n","    words = re.findall(r\"[A-Za-z]+\", sentence)\n","    cand = [w for w in words if w.lower() not in STOP]\n","    if not cand:\n","        return sentence, \"\"\n","    cand.sort(key=len, reverse=True)\n","    ans = cand[0]\n","    masked = re.sub(rf\"\\b{re.escape(ans)}\\b\", \"____\", sentence, 1)\n","    return masked, ans\n","\n","def generate_quiz(text, n=5):\n","    sents = [s for s in sent_tokenize(text) if len(s.split()) > 6]\n","    np.random.shuffle(sents)\n","    quiz = []\n","    for s in sents:\n","        q, a = make_fill_blank(s)\n","        if a:\n","            quiz.append((q, a))\n","            if len(quiz) >= n:\n","                break\n","    return quiz\n","\n","# ------------------ Demo Entry Point ------------------\n","def main():\n","    print(\"== StudyMate AI with Granite Loaded ==\")\n","\n","    sample_text = \"\"\"\n","    Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy. This energy is stored in the form of glucose, a sugar molecule. The process takes place in the chloroplasts of plant cells, where chlorophyll captures sunlight. Carbon dioxide from the air and water from the soil are used to produce glucose and oxygen. This oxygen is released into the atmosphere, making photosynthesis essential for life on Earth.\n","    \"\"\"\n","\n","    chunks = chunk_text(sample_text)\n","    embedder = load_embedder()\n","    embs, chunk_list = build_index(embedder, chunks)\n","\n","    tokenizer, model = load_granite()\n","\n","    query = \"How do plants produce oxygen?\"\n","    context_chunks = retrieve(query, embedder, embs, chunk_list, top_k=3)\n","    context = \" \".join([c[0] for c in context_chunks])\n","    prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n","    answer = granite_generate(prompt, tokenizer, model)\n","\n","    print(f\"\\nüß† Q&A\")\n","    print(f\"Question: {query}\")\n","    print(f\"Answer: {answer}\")\n","\n","    summary = summarize_text(sample_text, tokenizer, model)\n","    print(f\"\\nüìù Summary\")\n","    print(summary)\n","\n","    quiz = generate_quiz(sample_text)\n","    print(\"\\n‚úçÔ∏è Quiz\")\n","    for i, (q, a) in enumerate(quiz, 1):\n","        print(f\"Q{i}: {q}\")\n","        print(f\"A{i}: {a}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"_nXq_MTF3Z9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use a pipeline as a high-level helper\n","from transformers import pipeline\n","\n","pipe = pipeline(\"text-generation\", model=\"ibm-granite/granite-3.3-2b-instruct\")\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Who are you?\"},\n","]\n","pipe(messages)"],"metadata":{"id":"NATl1u3F2u4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-instruct\")\n","model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.3-2b-instruct\")\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Who are you?\"},\n","]\n","inputs = tokenizer.apply_chat_template(\n","\tmessages,\n","\tadd_generation_prompt=True,\n","\ttokenize=True,\n","\treturn_dict=True,\n","\treturn_tensors=\"pt\",\n",").to(model.device)\n","\n","outputs = model.generate(**inputs, max_new_tokens=40)\n","print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"],"metadata":{"id":"XWiFYlnq2u4p"},"execution_count":null,"outputs":[]}]}